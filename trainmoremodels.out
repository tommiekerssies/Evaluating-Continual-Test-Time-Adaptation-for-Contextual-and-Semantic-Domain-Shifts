WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.3779, 
      train accuracy: 0.88457572,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.1006, 
      train accuracy: 0.99478984,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0353, 
      train accuracy: 0.99835628,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0232, 
      train accuracy: 0.99892640,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0273, 
      train accuracy: 0.99874449,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0229, 
      train accuracy: 0.99882936,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0228, 
      train accuracy: 0.99887788,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0261, 
      train accuracy: 0.99870807,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0207, 
      train accuracy: 0.99889612,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0197, 
      train accuracy: 0.99899924,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 11], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.3779, 
      train accuracy: 0.88457572,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.1006, 
      train accuracy: 0.99478984,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0353, 
      train accuracy: 0.99835628,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0232, 
      train accuracy: 0.99892640,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0273, 
      train accuracy: 0.99874449,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0229, 
      train accuracy: 0.99882936,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0228, 
      train accuracy: 0.99887788,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0261, 
      train accuracy: 0.99870807,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0207, 
      train accuracy: 0.99889612,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0197, 
      train accuracy: 0.99899924,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 10, 11], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.5264, 
      train accuracy: 0.88048041,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.0899, 
      train accuracy: 0.99538285,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0309, 
      train accuracy: 0.99871224,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0171, 
      train accuracy: 0.99918598,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0118, 
      train accuracy: 0.99955964,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0111, 
      train accuracy: 0.99949956,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0212, 
      train accuracy: 0.99883902,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0230, 
      train accuracy: 0.99886572,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0224, 
      train accuracy: 0.99875897,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0247, 
      train accuracy: 0.99867225,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[1, 2, 3, 4, 5, 6, 7, 9, 10, 11], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.5566, 
      train accuracy: 0.87816662,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.0948, 
      train accuracy: 0.99503618,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0257, 
      train accuracy: 0.99904591,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0140, 
      train accuracy: 0.99951297,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0144, 
      train accuracy: 0.99934614,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0153, 
      train accuracy: 0.99936616,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0164, 
      train accuracy: 0.99919271,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0369, 
      train accuracy: 0.99790508,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0391, 
      train accuracy: 0.99797845,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0309, 
      train accuracy: 0.99832541,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[1, 2, 3, 4, 5, 6, 8, 9, 10, 11], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.5355, 
      train accuracy: 0.87954628,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.0941, 
      train accuracy: 0.99533612,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0327, 
      train accuracy: 0.99856550,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0150, 
      train accuracy: 0.99944621,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0132, 
      train accuracy: 0.99943954,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0118, 
      train accuracy: 0.99951291,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0173, 
      train accuracy: 0.99909925,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0679, 
      train accuracy: 0.99621683,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0418, 
      train accuracy: 0.99773145,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0273, 
      train accuracy: 0.99843872,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[1, 2, 3, 4, 5, 7, 8, 9, 10, 11], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.4987, 
      train accuracy: 0.88143182,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.0909, 
      train accuracy: 0.99537629,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0331, 
      train accuracy: 0.99855882,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0253, 
      train accuracy: 0.99879235,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0186, 
      train accuracy: 0.99911928,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0139, 
      train accuracy: 0.99937952,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0110, 
      train accuracy: 0.99952626,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0138, 
      train accuracy: 0.99933279,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0241, 
      train accuracy: 0.99872565,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0394, 
      train accuracy: 0.99783158,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[1, 2, 3, 4, 6, 7, 8, 9, 10, 11], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.5616, 
      train accuracy: 0.87722480,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.1041, 
      train accuracy: 0.99460971,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0300, 
      train accuracy: 0.99873251,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0216, 
      train accuracy: 0.99910605,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0208, 
      train accuracy: 0.99906605,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0138, 
      train accuracy: 0.99945962,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0107, 
      train accuracy: 0.99957973,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0148, 
      train accuracy: 0.99938625,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0139, 
      train accuracy: 0.99933958,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0224, 
      train accuracy: 0.99883920,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[1, 2, 3, 5, 6, 7, 8, 9, 10, 11], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.5467, 
      train accuracy: 0.87887239,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.1010, 
      train accuracy: 0.99458218,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0356, 
      train accuracy: 0.99843204,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0199, 
      train accuracy: 0.99918598,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0095, 
      train accuracy: 0.99967307,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0143, 
      train accuracy: 0.99932611,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0109, 
      train accuracy: 0.99957967,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0098, 
      train accuracy: 0.99959302,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0185, 
      train accuracy: 0.99897248,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0426, 
      train accuracy: 0.99760467,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[1, 2, 4, 5, 6, 7, 8, 9, 10, 11], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.6147, 
      train accuracy: 0.87432861,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.0980, 
      train accuracy: 0.99506253,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0265, 
      train accuracy: 0.99893242,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0154, 
      train accuracy: 0.99939948,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0110, 
      train accuracy: 0.99951959,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0076, 
      train accuracy: 0.99970645,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0070, 
      train accuracy: 0.99977982,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0228, 
      train accuracy: 0.99879235,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0401, 
      train accuracy: 0.99775815,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0326, 
      train accuracy: 0.99811178,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[1, 3, 4, 5, 6, 7, 8, 9, 10, 11], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.5188, 
      train accuracy: 0.88046438,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.1014, 
      train accuracy: 0.99466240,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0324, 
      train accuracy: 0.99846542,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0155, 
      train accuracy: 0.99946624,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0107, 
      train accuracy: 0.99959970,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0144, 
      train accuracy: 0.99937952,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0223, 
      train accuracy: 0.99881238,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0290, 
      train accuracy: 0.99845207,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0475, 
      train accuracy: 0.99725115,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0280, 
      train accuracy: 0.99845207,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 5
Namespace(path='./', max_epochs=10, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model=None, eval=None, train_sessions=[2, 3, 4, 5, 6, 7, 8, 9, 10, 11], val_sessions=None, ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
starting val accuracy: None

      epoch 0, 
      train loss: 2.5510, 
      train accuracy: 0.87846279,
      val accuracy: None
    

      epoch 1, 
      train loss: 0.1030, 
      train accuracy: 0.99469578,
      val accuracy: None
    

      epoch 2, 
      train loss: 0.0310, 
      train accuracy: 0.99867892,
      val accuracy: None
    

      epoch 3, 
      train loss: 0.0152, 
      train accuracy: 0.99940622,
      val accuracy: None
    

      epoch 4, 
      train loss: 0.0177, 
      train accuracy: 0.99926609,
      val accuracy: None
    

      epoch 5, 
      train loss: 0.0201, 
      train accuracy: 0.99904591,
      val accuracy: None
    

      epoch 6, 
      train loss: 0.0183, 
      train accuracy: 0.99916601,
      val accuracy: None
    

      epoch 7, 
      train loss: 0.0418, 
      train accuracy: 0.99778491,
      val accuracy: None
    

      epoch 8, 
      train loss: 0.0563, 
      train accuracy: 0.99671739,
      val accuracy: None
    

      epoch 9, 
      train loss: 0.0267, 
      train accuracy: 0.99861223,
      val accuracy: None
    
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,6,7,8,9,10_epoch_9_train_acc_0.9989992380142212.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647077 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647078 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647079 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647080 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647082 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647083 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647084 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 4 (pid: 647081) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:05:08
  host      : blade1.coc.org
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 647081)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,6,7,8,9,11_epoch_9_train_acc_0.9989992380142212.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647216 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647217 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647218 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647219 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647220 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647221 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647222 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 7 (pid: 647223) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:05:21
  host      : blade1.coc.org
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 647223)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,6,7,8,10,11_epoch_9_train_acc_0.9986722469329834.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647308 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647309 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647310 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647311 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647312 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647313 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647314 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 7 (pid: 647315) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:05:34
  host      : blade1.coc.org
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 647315)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,6,7,9,10,11_epoch_9_train_acc_0.9983254075050354.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647416 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647417 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647418 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647419 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647420 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647421 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647422 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 7 (pid: 647423) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:05:48
  host      : blade1.coc.org
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 647423)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,6,8,9,10,11_epoch_9_train_acc_0.9984387159347534.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647517 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647518 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647519 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647520 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647521 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647522 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647524 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 6 (pid: 647523) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:06:01
  host      : blade1.coc.org
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 647523)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,7,8,9,10,11_epoch_9_train_acc_0.9978315830230713.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647624 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647625 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647626 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647627 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647628 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647629 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647630 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 7 (pid: 647631) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:06:14
  host      : blade1.coc.org
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 647631)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,6,7,8,9,10,11_epoch_9_train_acc_0.9988391995429993.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647727 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647728 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647729 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647730 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647732 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647733 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647734 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 4 (pid: 647731) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:06:27
  host      : blade1.coc.org
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 647731)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,5,6,7,8,9,10,11_epoch_9_train_acc_0.9976046681404114.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647828 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647829 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647830 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647831 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647832 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647833 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647835 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 6 (pid: 647834) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:06:40
  host      : blade1.coc.org
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 647834)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,4,5,6,7,8,9,10,11_epoch_9_train_acc_0.9981117844581604.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647918 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647919 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647920 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647921 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647922 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647924 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 647925 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 5 (pid: 647923) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:06:54
  host      : blade1.coc.org
  rank      : 5 (local_rank: 5)
  exitcode  : 1 (pid: 647923)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,3,4,5,6,7,8,9,10,11_epoch_9_train_acc_0.9984520673751831.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648020 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648021 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648022 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648023 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648025 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648026 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648027 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 4 (pid: 648024) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:07:07
  host      : blade1.coc.org
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 648024)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=1024, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_2,3,4,5,6,7,8,9,10,11_epoch_9_train_acc_0.9986122250556946.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648121 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648122 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648123 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648124 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648126 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648127 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 648128 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 4 (pid: 648125) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:07:20
  host      : blade1.coc.org
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 648125)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
moreexperiments.run: 13: bn.py: not found
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=64, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,6,7,8,9,10_epoch_9_train_acc_0.9989992380142212.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650544 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650545 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650546 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650547 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650548 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650549 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650551 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 6 (pid: 650550) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:07:33
  host      : blade1.coc.org
  rank      : 6 (local_rank: 6)
  exitcode  : 1 (pid: 650550)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=64, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,6,7,8,9,11_epoch_9_train_acc_0.9989992380142212.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650646 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650647 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650648 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650649 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650650 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650651 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650652 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 7 (pid: 650653) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:07:46
  host      : blade1.coc.org
  rank      : 7 (local_rank: 7)
  exitcode  : 1 (pid: 650653)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=64, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,6,7,8,10,11_epoch_9_train_acc_0.9986722469329834.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650748 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650749 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650750 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650752 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650753 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650754 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 650755 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 650751) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:08:00
  host      : blade1.coc.org
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 650751)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018483"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018484"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018485"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018486"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018487"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018488"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018489"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018491"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018492"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018493"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=128, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,6,7,8,10,11_epoch_9_train_acc_0.9986722469329834.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651007 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651008 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651010 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651011 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651013 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651014 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651015 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 651009) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:08:24
  host      : blade1.coc.org
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 651009)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018507"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018508"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018509"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018510"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018512"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018513"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018514"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018515"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018516"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018517"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
World size: 8
Namespace(path='./', max_epochs=None, batch_size=256, lr=0.001, seed=0, num_workers=10, cycles=1, model='sessions_1,2,3,4,5,6,7,8,10,11_epoch_9_train_acc_0.9986722469329834.model', eval=True, train_sessions=None, val_sessions=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], ip=None, stdin=None, control=None, hb=None, shell=None, transport=None, iopub=None, f=None, **{'Session.signature_scheme': None, 'Session.key': None})
Traceback (most recent call last):
  File "/home/tommie_kerssies/CORe50/bn.py", line 4, in <module>
    model = utils.get_model(load_saved_model=True) 
  File "/home/tommie_kerssies/CORe50/utils.py", line 105, in get_model
    model = DDP(model, [dist.get_rank()], dist.get_rank())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 641, in __init__
    dist._verify_params_across_processes(self.process_group, parameters)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651301 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651302 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651303 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651304 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651307 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651308 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 651309 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 4 (pid: 651305) of binary: /home/tommie_kerssies/miniconda3/envs/DAGM/bin/python
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 245, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
bn.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-06-23_23:08:48
  host      : blade1.coc.org
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 651305)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018532"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018533"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018534"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018535"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018536"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018537"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018538"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:401] [c10d] The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use).
[W socket.cpp:401] [c10d] The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
[E socket.cpp:435] [c10d] The server socket has failed to listen on any local network address.
ERROR:torch.distributed.elastic.multiprocessing.errors.error_handler:{
  "message": {
    "message": "RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).",
    "extraInfo": {
      "py_callstack": "Traceback (most recent call last):\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 345, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 724, in main\n    run(args)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py\", line 715, in run\n    elastic_launch(\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 131, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 236, in launch_agent\n    result = agent.run()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 709, in run\n    result = self._invoke_run(role)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 844, in _invoke_run\n    self._initialize_workers(self._worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 678, in _initialize_workers\n    self._rendezvous(worker_group)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py\", line 125, in wrapper\n    result = f(*args, **kwargs)\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py\", line 538, in _rendezvous\n    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()\n  File \"/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py\", line 55, in next_rendezvous\n    self._store = TCPStore(  # type: ignore[call-arg]\nRuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).\n",
      "timestamp": "1656018540"
    }
  }
}
Traceback (most recent call last):
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==1.11.0', 'console_scripts', 'torchrun')())
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 345, in wrapper
    return f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 724, in main
    run(args)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/run.py", line 715, in run
    elastic_launch(
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 131, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 236, in launch_agent
    result = agent.run()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 844, in _invoke_run
    self._initialize_workers(self._worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 678, in _initialize_workers
    self._rendezvous(worker_group)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/metrics/api.py", line 125, in wrapper
    result = f(*args, **kwargs)
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/agent/server/api.py", line 538, in _rendezvous
    store, group_rank, group_world_size = spec.rdzv_handler.next_rendezvous()
  File "/home/tommie_kerssies/miniconda3/envs/DAGM/lib/python3.9/site-packages/torch/distributed/elastic/rendezvous/static_tcp_rendezvous.py", line 55, in next_rendezvous
    self._store = TCPStore(  # type: ignore[call-arg]
RuntimeError: The server socket has failed to listen on any local network address. The server socket has failed to bind to [::]:29500 (errno: 98 - Address already in use). The server socket has failed to bind to ?UNKNOWN? (errno: 98 - Address already in use).
